# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /var/log/flume/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/applog/log/app.*.log

# Describe the sink
a1.sinks.k1.type = hdfs
# 数据生成的目录 根据时间戳来生成
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/2025-03-01/
# 生成的数据前缀
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.filePrefix = events-
# 多长时间生成一个文件，单位是秒
a1.sinks.k1.hdfs.rollInterval = 30
# 多大的数据量生成一个文件，单位是字节
a1.sinks.k1.hdfs.rollSize = 134217728
# 多少条数据生成一个文件，单位是个，如果是0，则表示不限制
a1.sinks.k1.hdfs.rollCount = 0
# 每次获取数据量条数
a1.sinks.k1.hdfs.batchSize = 100
# 生成文件的格式  SequenceFile 序列化的文件（默认值）, DataStream普通文本 or CompressedStream压缩格式
a1.sinks.k1.hdfs.fileType = CompressedStream
# 压缩格式 需要配合 fileType = CompressedStream一起使用，gzip, bzip2, lzo, lzop, snappy
a1.sinks.k1.hdfs.codeC = lzo
# 是否使用本地的时间戳
# 如果为false，则使用flume的event中header里面的 timestamp ，如果没有则报错
# 如果为true，则直接使用linux中的本地系统时间
a1.sinks.k1.hdfs.useLocalTimeStamp = true


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000